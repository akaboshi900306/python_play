{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTING THE NECESSARY LIBRARIES\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import os.path\n",
    "from subprocess import Popen\n",
    "from datetime import datetime\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING THE REQUIRED FUNCTIONS\n",
    "\n",
    "#function to instantiate Google Cloud credentials and bigquery service\n",
    "\n",
    "def create_service(svc_type='bigquery'):\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    return discovery.build(svc_type, 'v2', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to poll job untill it is complete\n",
    "\n",
    "def poll_job(bigquery, job):\n",
    "    print('Waiting for job to finish...')\n",
    "\n",
    "    request = bigquery.jobs().get(\n",
    "        projectId=job['jobReference']['projectId'],\n",
    "        jobId=job['jobReference']['jobId'])\n",
    "\n",
    "    while True:\n",
    "        result = request.execute(num_retries=2)\n",
    "\n",
    "        if result['status']['state'] == 'DONE':\n",
    "            if 'errorResult' in result['status']:\n",
    "                raise RuntimeError(result['status']['errorResult'])\n",
    "            print('Job complete.')\n",
    "            return\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# function to download the necessary csv files\n",
    "            \n",
    "\n",
    "def download_from_storage(uri , dest_dir):\n",
    "\n",
    "        # Downloads all files in a Google cloud storage bucket/directory to a local destination \n",
    "        #\n",
    "        # Inputs: uri - string - URI of the google cloud storage bucket that contains the target files\n",
    "        #          dest_dir - string - absolute path of the directory in which the files should be placed\n",
    "        #\n",
    "        # Outputs: Boolean True if download is successful; False O.W.\n",
    "        #\n",
    "        # NOTE: This method is open to code injection and should be re-written to eliminate that risk\n",
    "        #\n",
    "        #        This method requires that the user's .boto configuration file limit the number of threads. The file\n",
    "        #            is located in: \n",
    "        #                C:\\Users\\ --LDAP_ID-- \\AppData\\Roaming\\gcloud\\legacy_credentials\\ --firstname_lastname@homedepot.com-- \\\n",
    "        #            \n",
    "        #            The file should contain the following lines:\n",
    "        #                [GSUtil]\n",
    "        #                parallel_thread_count = 3\n",
    "        #\n",
    "        from datetime import datetime\n",
    "        from os import stat, mkdir, path\n",
    "\n",
    "        command = \"gsutil -m cp \" + uri +' .'#-m\n",
    "        #args = split(commmand)\n",
    "\n",
    "        print(\"Pull beginning at \" + str(datetime.now()))\n",
    "\n",
    "        directory = path.dirname(dest_dir)\n",
    "\n",
    "        try:\n",
    "            stat(directory)\n",
    "        except:\n",
    "            mkdir(directory) \n",
    "\n",
    "        p = Popen(command, shell = True, cwd = dest_dir)#args, )\n",
    "        p.wait()\n",
    "        \n",
    "        return \"Pull completed at \" + str(datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clear the directory in the cloud storage\n",
    "def clear_storage_dir(uri, conf = True):\n",
    "    # Clears all files from the specified Cloud Storage directory\n",
    "    # # CAUTION: All actions are irreversible; ensure your URI is correct\n",
    "    # #\n",
    "    # # Inputs: uri - string - URI of the directory to be cleared\n",
    "    # #\n",
    "    # # Outputs: Boolean True if BAT successfully run; False O.W.\n",
    "    # #\n",
    "    \n",
    "    command = 'gsutil rm -r ' + uri+' .'\n",
    "    print(command)\n",
    "    if conf:\n",
    "        confirmation = input('You are about to delete the following URI:\\n' + uri +\"\\nPlease type 'DELETE' to confirm:\")\n",
    "        \n",
    "        if confirmation!='DELETE':\n",
    "            print(\"Delete aborted\")\n",
    "            return False\n",
    "    \n",
    "    p = Popen(command, shell = True)\n",
    "    return p.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download the multiple csv files as one DataFrame in python\n",
    "\n",
    "def fil_cons_csv(path):\n",
    "        allFiles = glob.glob(path + \"*.csv\")\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            df = pd.read_csv(file_)\n",
    "            list_.append(df)\n",
    "        frame = pd.concat(list_,ignore_index=True)\n",
    "        return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to upload the csv file to cloud storage\n",
    "    \n",
    "def upload_csv(project_id,filepath,wrkg_dir,filename=None,out_fmt=None):\n",
    "    if filename == None:\n",
    "        rpl = wrkg_dir+'/'\n",
    "        filename = filepath.replace(rpl,'')\n",
    "    else:\n",
    "        if out_fmt == None:\n",
    "            filename = filename + '.csv'\n",
    "        else:\n",
    "            filename = filename + '.' + out_fmt\n",
    "    bucket = 'sc-lab-data/PXJ06AA'\n",
    "    gs_uri = 'gs://'+bucket+'/'+filename   \n",
    "    print(gs_uri)\n",
    "    #cmd = 'gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp '+filepath+' '+gs_uri\n",
    "    prcss = Popen(['gsutil','-o','GSUtil:parallel_composite_upload_threshold=150M','cp',filepath,gs_uri],bufsize=-1,shell=True)\n",
    "    prcss.wait()\n",
    "    if prcss.returncode == 0:\n",
    "        os.remove(filepath)\n",
    "        prcss.terminate()\n",
    "        return gs_uri\n",
    "    else:\n",
    "        prcss.terminate()\n",
    "        return 'Failed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load table from cloud to big query\n",
    "    \n",
    "def load_table(bigquery, project_id, dataset_id, table_name,\n",
    "               source_path,schema,write_mode=None):\n",
    "    if write_mode == None:\n",
    "        wmode = 'WRITE_TRUNCATE'\n",
    "    elif write_mode.lower() == 'wa':\n",
    "        wmode = 'WRITE_APPEND'\n",
    "    elif write_mode.lower() == 'we':\n",
    "        wmode = 'WRITE_EMPTY'\n",
    "    else:\n",
    "        wmode = 'WRITE_TRUNCATE'\n",
    "    job_data = {\n",
    "        'jobReference': {\n",
    "            'projectId': project_id,\n",
    "            'jobId': str(uuid.uuid4())\n",
    "        },\n",
    "        'configuration': {\n",
    "            'load': {\n",
    "                'sourceUris': [source_path],\n",
    "                'schema': {\n",
    "                    'fields': schema\n",
    "                },\n",
    "                'createDisposition':'CREATE_IF_NEEDED',\n",
    "                'writeDisposition' : wmode,\n",
    "                'fieldDelimiter':',',\n",
    "                'skipLeadingRows': 1,\n",
    "                'quote':'~',\n",
    "                'destinationTable': {\n",
    "                    'projectId': project_id,\n",
    "                    'datasetId': dataset_id,\n",
    "                    'tableId': table_name\n",
    "                },\n",
    "                'ignoreUnknownValues':True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return bigquery.jobs().insert(\n",
    "        projectId=project_id,\n",
    "        body=job_data).execute(num_retries=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get schema\n",
    "\n",
    "def get_bq_table_schema(bigquery,dataset_id,table_id,project_id = 'analytics-supplychain-thd'):\n",
    "    \n",
    "\n",
    "\n",
    "    request = bigquery.tables().get(projectId = project_id,datasetId = dataset_id, tableId = table_id)\n",
    "    results = request.execute(num_retries = 100)\n",
    "    \n",
    "    return results['schema']['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STARTING THE DATA TRANSFER FROM BIG QUERY TO PANDAS DATAFRAMES  \n",
    "# transferring data from big_query to cloud storage\n",
    "\n",
    "def fetch_data():\n",
    "    try:\n",
    "        clear_storage_dir(dest_bucket+'/*', conf = False)\n",
    "    except:\n",
    "        var = None\n",
    "    job_data1 = {\n",
    "            'jobReference':{\n",
    "                'projectId':'analytics-supplychain-thd',\n",
    "                'jobId':str(uuid.uuid4())\n",
    "                },\n",
    "            'configuration':{\n",
    "                'extract':{\n",
    "                    'sourceTable': {\n",
    "                        'projectId':bq_project\n",
    "                        ,'datasetId':bq_target_dataset\n",
    "                        ,'tableId':bq_target_table\n",
    "                    }\n",
    "                    ,'destinationUri':dest_bucket + '/' + dest_fn\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    bq_conn = create_service()\n",
    "    export_job = bq_conn.jobs().insert(projectId='analytics-supplychain-thd',body=job_data1).execute(num_retries=5)\n",
    "    #download the data into csv files\n",
    "    poll_job(bq_conn,export_job)\n",
    "\n",
    "    download_from_storage(dest_bucket+\"/*\", download_dir)      \n",
    "    clear_storage_dir(dest_bucket,False)\n",
    "\n",
    "    #reading the csv files\n",
    "    print(\"data fetch start - \"+ str(datetime.now()))\n",
    "    print('reading the data from the csv file')\n",
    "    DATA=fil_cons_csv(download_dir)\n",
    "    #DATA=DATA[DATA['CLASS']==3]\n",
    "    print(\"data fetch complete - \"+ str(datetime.now()))\n",
    "    return DATA\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE0__20200102094042064889/* .\n",
      "Waiting for job to finish...\n",
      "Job complete.\n",
      "Pull beginning at 2020-01-02 09:41:45.551020\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE0__20200102094042064889 .\n",
      "data fetch start - 2020-01-02 09:42:06.361012\n",
      "reading the data from the csv file\n",
      "data fetch complete - 2020-01-02 09:42:14.205121\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE1__20200102094330130759/* .\n",
      "Waiting for job to finish...\n",
      "Job complete.\n",
      "Pull beginning at 2020-01-02 09:44:44.569778\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE1__20200102094330130759 .\n",
      "data fetch start - 2020-01-02 09:45:16.414710\n",
      "reading the data from the csv file\n",
      "data fetch complete - 2020-01-02 09:45:25.022212\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE2__20200102094642817507/* .\n",
      "Waiting for job to finish...\n",
      "Job complete.\n",
      "Pull beginning at 2020-01-02 09:47:59.750044\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE2__20200102094642817507 .\n",
      "data fetch start - 2020-01-02 09:48:29.046192\n",
      "reading the data from the csv file\n",
      "data fetch complete - 2020-01-02 09:48:37.198859\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE3__20200102094954343893/* .\n",
      "Waiting for job to finish...\n",
      "Job complete.\n",
      "Pull beginning at 2020-01-02 09:51:01.686800\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE3__20200102094954343893 .\n",
      "data fetch start - 2020-01-02 09:51:33.444078\n",
      "reading the data from the csv file\n",
      "data fetch complete - 2020-01-02 09:51:40.933931\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE4__20200102095255052383/* .\n",
      "Waiting for job to finish...\n",
      "Job complete.\n",
      "Pull beginning at 2020-01-02 09:54:00.127347\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ASL_SSTK_DMND_NULLS_WEB_FILE4__20200102095255052383 .\n",
      "data fetch start - 2020-01-02 09:54:25.734802\n",
      "reading the data from the csv file\n",
      "data fetch complete - 2020-01-02 09:54:33.431897\n"
     ]
    }
   ],
   "source": [
    "#### INPUT DIRECTORIES AND PARAMETERS\n",
    "\n",
    "file_name=['ASL_SSTK_DMND_NULLS_WEB_FILE0','ASL_SSTK_DMND_NULLS_WEB_FILE1','ASL_SSTK_DMND_NULLS_WEB_FILE2','ASL_SSTK_DMND_NULLS_WEB_FILE3','ASL_SSTK_DMND_NULLS_WEB_FILE4']\n",
    "upload_file=['/Users/CXL6PMD/Documents/ASL_Override/Data/UPLOAD_FILE/WEB/upload_file0.csv','/Users/CXL6PMD/Documents/ASL_Override/Data/UPLOAD_FILE/WEB/upload_file1.csv','/Users/CXL6PMD/Documents/ASL_Override/Data/UPLOAD_FILE/WEB/upload_file2.csv','/Users/CXL6PMD/Documents/ASL_Override/Data/UPLOAD_FILE/WEB/upload_file3.csv','/Users/CXL6PMD/Documents/ASL_Override/Data/UPLOAD_FILE/WEB/upload_file4.csv']\n",
    "# details of the big query table\n",
    "for i in range(5):\n",
    "    bq_table= file_name[i]\n",
    "    bq_project='analytics-supplychain-thd'\n",
    "    bq_target_dataset='PXJ06AA_ASL2'\n",
    "    bq_target_table =file_name[i]\n",
    "    now = datetime.now()\n",
    "    bucket_now = now.strftime('%Y%m%d%H%M%S%f')\n",
    "    \n",
    "    #details of the google cloud storage bucket\n",
    "    \n",
    "    dest_bucket = 'gs://sc-lab-data/PXJ06AA/'+bq_table+'__'+bucket_now\n",
    "    dest_fn = 'OPTDATA_csv_*.csv'\n",
    "    \n",
    "    #details to download the optimization data into the scratch disk\n",
    "    \n",
    "    scratch_disk = '/Users/CXL6PMD/Documents'\n",
    "    \n",
    "    download_dir = scratch_disk + '/ASL_Override/Data/BQTOPYTHON_OPT_'+bucket_now+'/'\n",
    "    project_id='analytics-supplychain-thd'\n",
    "    \n",
    "    DATA = fetch_data()\n",
    "    DATA.to_csv(upload_file[i], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ITEMLOC_SVLVL_SSTK_D30C22_OVERRIDE__20200102095548143631/* .\n",
      "Waiting for job to finish...\n",
      "Job complete.\n",
      "Pull beginning at 2020-01-02 09:56:12.950729\n",
      "gsutil rm -r gs://sc-lab-data/PXJ06AA/ITEMLOC_SVLVL_SSTK_D30C22_OVERRIDE__20200102095548143631 .\n",
      "data fetch start - 2020-01-02 09:56:21.486730\n",
      "reading the data from the csv file\n",
      "data fetch complete - 2020-01-02 09:56:22.902006\n"
     ]
    }
   ],
   "source": [
    "#### INPUT DIRECTORIES AND PARAMETERS\n",
    "\n",
    "file_name=['ITEMLOC_SVLVL_SSTK_D30C22_OVERRIDE']\n",
    "upload_file=['/Users/CXL6PMD/Documents/ASL_Override/Data/UPLOAD_FILE/WEB/upload_file5.csv']\n",
    "# details of the big query table\n",
    "for i in range(1):\n",
    "    bq_table= file_name[i]\n",
    "    bq_project='analytics-supplychain-thd'\n",
    "    bq_target_dataset='CXL6PMD_SF_EFFECT'\n",
    "    bq_target_table =file_name[i]\n",
    "    now = datetime.now()\n",
    "    bucket_now = now.strftime('%Y%m%d%H%M%S%f')\n",
    "    \n",
    "    #details of the google cloud storage bucket\n",
    "    \n",
    "    dest_bucket = 'gs://sc-lab-data/PXJ06AA/'+bq_table+'__'+bucket_now\n",
    "    dest_fn = 'OPTDATA_csv_*.csv'\n",
    "    \n",
    "    #details to download the optimization data into the scratch disk\n",
    "    \n",
    "    scratch_disk = '/Users/CXL6PMD/Documents'\n",
    "    \n",
    "    download_dir = scratch_disk + '/ASL_Override/Data/BQTOPYTHON_OPT_'+bucket_now+'/'\n",
    "    project_id='analytics-supplychain-thd'\n",
    "    \n",
    "    DATA = fetch_data()\n",
    "    DATA.to_csv(upload_file[i], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
